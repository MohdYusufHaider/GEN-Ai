{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66e2ec3-65f9-4644-b56d-679d52b40202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Write a code to generate a random sentence using probabilistic modeling\n",
    "#(Markov Chain). Use the sentence \"The cat is on the mat\" as an exampl?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68022461-56ee-4c11-84d9-c85b53a77bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence: is on the mat\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def build_markov_chain(text):\n",
    "    words = text.split()\n",
    "    chain = {}\n",
    "    for i in range(len(words) - 1):\n",
    "        word = words[i]\n",
    "        next_word = words[i + 1]\n",
    "        chain.setdefault(word, []).append(next_word)\n",
    "    return chain\n",
    "\n",
    "def generate_sentence(chain, length=10):\n",
    "    word = random.choice(list(chain.keys()))\n",
    "    sentence = [word]\n",
    "    for _ in range(length - 1):\n",
    "        next_words = chain.get(word, None)\n",
    "        if not next_words:\n",
    "            break\n",
    "        word = random.choice(next_words)\n",
    "        sentence.append(word)\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "text = \"The cat is on the mat\"\n",
    "markov_chain = build_markov_chain(text)\n",
    "random_sentence = generate_sentence(markov_chain, length=6)\n",
    "print(\"Generated sentence:\", random_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ac50b5-55fd-47fa-9b04-d07c6e3370e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Build a simple Autoencoder model using Keras to learn a compressed\n",
    "#representation of a given sentence. Use a dataset of your choice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3ba11c-0aa7-4a57-abda-ed3ffc9001c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0865 - val_loss: 0.0834\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0832 - val_loss: 0.0831\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0828 - val_loss: 0.0828\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0820 - val_loss: 0.0823\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0813 - val_loss: 0.0814\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0800 - val_loss: 0.0802\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0786 - val_loss: 0.0788\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0764 - val_loss: 0.0774\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0754 - val_loss: 0.0758\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0735 - val_loss: 0.0743\n",
      "Autoencoder built successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "\n",
    "input_dim = 100 \n",
    "encoding_dim = 32  \n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Example training\n",
    "import numpy as np\n",
    "data = np.random.rand(1000, input_dim)  # Random dataset for demonstration\n",
    "autoencoder.fit(data, data, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "encoder = Model(input_layer, encoded)\n",
    "print(\"Autoencoder built successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68bd559-bbe3-4a85-8774-6a304a2e38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Use the Hugging Face transformers library to fine-tune a pre-trained GPT-2\n",
    "#model on a custom text data and generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e1bf2fc-e14f-4d6c-b138-e0db6b972842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Implement a text generation model using a simple Recurrent Neural\n",
    "##Network (RNN) in Keras. Train the model on a custom data and generate a\n",
    "#word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a02b18-3daf-4bd3-b77d-5e30503149cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 2s - 2s/step - accuracy: 0.0000e+00 - loss: 2.8442\n",
      "Epoch 2/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.0000e+00 - loss: 2.8267\n",
      "Epoch 3/100\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.0769 - loss: 2.8092\n",
      "Epoch 4/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.2308 - loss: 2.7915\n",
      "Epoch 5/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.2308 - loss: 2.7736\n",
      "Epoch 6/100\n",
      "1/1 - 0s - 88ms/step - accuracy: 0.3077 - loss: 2.7552\n",
      "Epoch 7/100\n",
      "1/1 - 0s - 85ms/step - accuracy: 0.3846 - loss: 2.7361\n",
      "Epoch 8/100\n",
      "1/1 - 0s - 84ms/step - accuracy: 0.3846 - loss: 2.7162\n",
      "Epoch 9/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.3846 - loss: 2.6953\n",
      "Epoch 10/100\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.3846 - loss: 2.6732\n",
      "Epoch 11/100\n",
      "1/1 - 0s - 116ms/step - accuracy: 0.3846 - loss: 2.6497\n",
      "Epoch 12/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.4615 - loss: 2.6247\n",
      "Epoch 13/100\n",
      "1/1 - 0s - 82ms/step - accuracy: 0.3846 - loss: 2.5980\n",
      "Epoch 14/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.3846 - loss: 2.5695\n",
      "Epoch 15/100\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.3846 - loss: 2.5390\n",
      "Epoch 16/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.3846 - loss: 2.5066\n",
      "Epoch 17/100\n",
      "1/1 - 0s - 94ms/step - accuracy: 0.3846 - loss: 2.4722\n",
      "Epoch 18/100\n",
      "1/1 - 0s - 72ms/step - accuracy: 0.4615 - loss: 2.4358\n",
      "Epoch 19/100\n",
      "1/1 - 0s - 95ms/step - accuracy: 0.4615 - loss: 2.3975\n",
      "Epoch 20/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4615 - loss: 2.3574\n",
      "Epoch 21/100\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4615 - loss: 2.3157\n",
      "Epoch 22/100\n",
      "1/1 - 0s - 91ms/step - accuracy: 0.4615 - loss: 2.2724\n",
      "Epoch 23/100\n",
      "1/1 - 0s - 63ms/step - accuracy: 0.4615 - loss: 2.2278\n",
      "Epoch 24/100\n",
      "1/1 - 0s - 83ms/step - accuracy: 0.4615 - loss: 2.1818\n",
      "Epoch 25/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.4615 - loss: 2.1342\n",
      "Epoch 26/100\n",
      "1/1 - 0s - 88ms/step - accuracy: 0.4615 - loss: 2.0846\n",
      "Epoch 27/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.5385 - loss: 2.0327\n",
      "Epoch 28/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.5385 - loss: 1.9780\n",
      "Epoch 29/100\n",
      "1/1 - 0s - 65ms/step - accuracy: 0.5385 - loss: 1.9200\n",
      "Epoch 30/100\n",
      "1/1 - 0s - 87ms/step - accuracy: 0.6154 - loss: 1.8585\n",
      "Epoch 31/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.6923 - loss: 1.7934\n",
      "Epoch 32/100\n",
      "1/1 - 0s - 67ms/step - accuracy: 0.6923 - loss: 1.7249\n",
      "Epoch 33/100\n",
      "1/1 - 0s - 63ms/step - accuracy: 0.6923 - loss: 1.6537\n",
      "Epoch 34/100\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.6923 - loss: 1.5804\n",
      "Epoch 35/100\n",
      "1/1 - 0s - 65ms/step - accuracy: 0.7692 - loss: 1.5060\n",
      "Epoch 36/100\n",
      "1/1 - 0s - 71ms/step - accuracy: 0.8462 - loss: 1.4316\n",
      "Epoch 37/100\n",
      "1/1 - 0s - 91ms/step - accuracy: 0.9231 - loss: 1.3581\n",
      "Epoch 38/100\n",
      "1/1 - 0s - 120ms/step - accuracy: 1.0000 - loss: 1.2863\n",
      "Epoch 39/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 1.0000 - loss: 1.2168\n",
      "Epoch 40/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 1.0000 - loss: 1.1498\n",
      "Epoch 41/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 1.0000 - loss: 1.0851\n",
      "Epoch 42/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 1.0000 - loss: 1.0226\n",
      "Epoch 43/100\n",
      "1/1 - 0s - 96ms/step - accuracy: 1.0000 - loss: 0.9621\n",
      "Epoch 44/100\n",
      "1/1 - 0s - 113ms/step - accuracy: 1.0000 - loss: 0.9035\n",
      "Epoch 45/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 1.0000 - loss: 0.8467\n",
      "Epoch 46/100\n",
      "1/1 - 0s - 104ms/step - accuracy: 1.0000 - loss: 0.7917\n",
      "Epoch 47/100\n",
      "1/1 - 0s - 72ms/step - accuracy: 1.0000 - loss: 0.7389\n",
      "Epoch 48/100\n",
      "1/1 - 0s - 83ms/step - accuracy: 1.0000 - loss: 0.6883\n",
      "Epoch 49/100\n",
      "1/1 - 0s - 100ms/step - accuracy: 1.0000 - loss: 0.6401\n",
      "Epoch 50/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 1.0000 - loss: 0.5944\n",
      "Epoch 51/100\n",
      "1/1 - 0s - 74ms/step - accuracy: 1.0000 - loss: 0.5514\n",
      "Epoch 52/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 1.0000 - loss: 0.5109\n",
      "Epoch 53/100\n",
      "1/1 - 0s - 176ms/step - accuracy: 1.0000 - loss: 0.4731\n",
      "Epoch 54/100\n",
      "1/1 - 0s - 82ms/step - accuracy: 1.0000 - loss: 0.4379\n",
      "Epoch 55/100\n",
      "1/1 - 0s - 65ms/step - accuracy: 1.0000 - loss: 0.4053\n",
      "Epoch 56/100\n",
      "1/1 - 0s - 104ms/step - accuracy: 1.0000 - loss: 0.3753\n",
      "Epoch 57/100\n",
      "1/1 - 0s - 66ms/step - accuracy: 1.0000 - loss: 0.3476\n",
      "Epoch 58/100\n",
      "1/1 - 0s - 72ms/step - accuracy: 1.0000 - loss: 0.3223\n",
      "Epoch 59/100\n",
      "1/1 - 0s - 96ms/step - accuracy: 1.0000 - loss: 0.2991\n",
      "Epoch 60/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 1.0000 - loss: 0.2780\n",
      "Epoch 61/100\n",
      "1/1 - 0s - 66ms/step - accuracy: 1.0000 - loss: 0.2587\n",
      "Epoch 62/100\n",
      "1/1 - 0s - 77ms/step - accuracy: 1.0000 - loss: 0.2410\n",
      "Epoch 63/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 1.0000 - loss: 0.2250\n",
      "Epoch 64/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 1.0000 - loss: 0.2103\n",
      "Epoch 65/100\n",
      "1/1 - 0s - 93ms/step - accuracy: 1.0000 - loss: 0.1968\n",
      "Epoch 66/100\n",
      "1/1 - 0s - 91ms/step - accuracy: 1.0000 - loss: 0.1845\n",
      "Epoch 67/100\n",
      "1/1 - 0s - 94ms/step - accuracy: 1.0000 - loss: 0.1733\n",
      "Epoch 68/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 1.0000 - loss: 0.1630\n",
      "Epoch 69/100\n",
      "1/1 - 0s - 88ms/step - accuracy: 1.0000 - loss: 0.1535\n",
      "Epoch 70/100\n",
      "1/1 - 0s - 98ms/step - accuracy: 1.0000 - loss: 0.1448\n",
      "Epoch 71/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 1.0000 - loss: 0.1368\n",
      "Epoch 72/100\n",
      "1/1 - 0s - 97ms/step - accuracy: 1.0000 - loss: 0.1294\n",
      "Epoch 73/100\n",
      "1/1 - 0s - 92ms/step - accuracy: 1.0000 - loss: 0.1226\n",
      "Epoch 74/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 1.0000 - loss: 0.1162\n",
      "Epoch 75/100\n",
      "1/1 - 0s - 88ms/step - accuracy: 1.0000 - loss: 0.1104\n",
      "Epoch 76/100\n",
      "1/1 - 0s - 107ms/step - accuracy: 1.0000 - loss: 0.1049\n",
      "Epoch 77/100\n",
      "1/1 - 0s - 84ms/step - accuracy: 1.0000 - loss: 0.0999\n",
      "Epoch 78/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 1.0000 - loss: 0.0952\n",
      "Epoch 79/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 1.0000 - loss: 0.0908\n",
      "Epoch 80/100\n",
      "1/1 - 0s - 99ms/step - accuracy: 1.0000 - loss: 0.0868\n",
      "Epoch 81/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 1.0000 - loss: 0.0830\n",
      "Epoch 82/100\n",
      "1/1 - 0s - 100ms/step - accuracy: 1.0000 - loss: 0.0795\n",
      "Epoch 83/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 1.0000 - loss: 0.0762\n",
      "Epoch 84/100\n",
      "1/1 - 0s - 96ms/step - accuracy: 1.0000 - loss: 0.0731\n",
      "Epoch 85/100\n",
      "1/1 - 0s - 82ms/step - accuracy: 1.0000 - loss: 0.0702\n",
      "Epoch 86/100\n",
      "1/1 - 0s - 81ms/step - accuracy: 1.0000 - loss: 0.0675\n",
      "Epoch 87/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 1.0000 - loss: 0.0650\n",
      "Epoch 88/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 1.0000 - loss: 0.0626\n",
      "Epoch 89/100\n",
      "1/1 - 0s - 67ms/step - accuracy: 1.0000 - loss: 0.0604\n",
      "Epoch 90/100\n",
      "1/1 - 0s - 90ms/step - accuracy: 1.0000 - loss: 0.0583\n",
      "Epoch 91/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 1.0000 - loss: 0.0563\n",
      "Epoch 92/100\n",
      "1/1 - 0s - 100ms/step - accuracy: 1.0000 - loss: 0.0544\n",
      "Epoch 93/100\n",
      "1/1 - 0s - 102ms/step - accuracy: 1.0000 - loss: 0.0527\n",
      "Epoch 94/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 1.0000 - loss: 0.0510\n",
      "Epoch 95/100\n",
      "1/1 - 0s - 121ms/step - accuracy: 1.0000 - loss: 0.0494\n",
      "Epoch 96/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 1.0000 - loss: 0.0480\n",
      "Epoch 97/100\n",
      "1/1 - 0s - 63ms/step - accuracy: 1.0000 - loss: 0.0465\n",
      "Epoch 98/100\n",
      "1/1 - 0s - 76ms/step - accuracy: 1.0000 - loss: 0.0452\n",
      "Epoch 99/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 1.0000 - loss: 0.0439\n",
      "Epoch 100/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 1.0000 - loss: 0.0427\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Generated Text: Once upon a courage over was a king who ruled over a great\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Step 1: Prepare the custom dataset\n",
    "def prepare_dataset(text, max_sequence_length=5):\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    sequences = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "    # Create input-output pairs\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequences) - max_sequence_length):\n",
    "        X.append(sequences[i:i + max_sequence_length])\n",
    "        y.append(sequences[i + max_sequence_length])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y, tokenizer, len(tokenizer.word_index) + 1\n",
    "\n",
    "# Example custom text data\n",
    "text_data = \"Once upon a time there was a king who ruled over a great kingdom with wisdom and courage.\"\n",
    "\n",
    "# Prepare the dataset\n",
    "max_sequence_length = 5\n",
    "X, y, tokenizer, vocab_size = prepare_dataset(text_data, max_sequence_length)\n",
    "\n",
    "# Step 2: Build the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=10), \n",
    "    SimpleRNN(64, return_sequences=False),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Step 3: Train the model\n",
    "model.fit(X, y, epochs=100, verbose=2)\n",
    "\n",
    "# Step 4: Generate text\n",
    "def generate_text(prompt, num_words, model, tokenizer, max_sequence_length):\n",
    "  \n",
    "    result = prompt\n",
    "    for _ in range(num_words):\n",
    "        # Tokenize the prompt\n",
    "        encoded = tokenizer.texts_to_sequences([result.split()])[-1]\n",
    "        encoded = pad_sequences([encoded], maxlen=max_sequence_length, truncating='pre')\n",
    "\n",
    "        # Predict the next word\n",
    "        prediction = np.argmax(model.predict(encoded), axis=-1)\n",
    "        next_word = tokenizer.index_word[prediction[0]]\n",
    "\n",
    "        # Append the predicted word to the result\n",
    "        result += \" \" + next_word\n",
    "\n",
    "    return result\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Once upon a\"\n",
    "generated_text = generate_text(prompt, num_words=10, model=model, tokenizer=tokenizer, max_sequence_length=max_sequence_length)\n",
    "print(f\"Generated Text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf0f6b00-5911-4a1a-bcfc-69e25cbf2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.Write a program to generate a sequence of text using an LSTM-based\n",
    "#model in TensorFlow, trained on a custom data of sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "095ae174-65e9-4722-9049-705121d0b915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 3s - 3s/step - accuracy: 0.0000e+00 - loss: 2.9461\n",
      "Epoch 2/100\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.0870 - loss: 2.9413\n",
      "Epoch 3/100\n",
      "1/1 - 0s - 85ms/step - accuracy: 0.1304 - loss: 2.9380\n",
      "Epoch 4/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 0.2609 - loss: 2.9314\n",
      "Epoch 5/100\n",
      "1/1 - 0s - 84ms/step - accuracy: 0.2174 - loss: 2.9257\n",
      "Epoch 6/100\n",
      "1/1 - 0s - 89ms/step - accuracy: 0.1739 - loss: 2.9218\n",
      "Epoch 7/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.1739 - loss: 2.9185\n",
      "Epoch 8/100\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.1739 - loss: 2.9127\n",
      "Epoch 9/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.1739 - loss: 2.9059\n",
      "Epoch 10/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.1739 - loss: 2.8963\n",
      "Epoch 11/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 0.1739 - loss: 2.8921\n",
      "Epoch 12/100\n",
      "1/1 - 0s - 72ms/step - accuracy: 0.1739 - loss: 2.8889\n",
      "Epoch 13/100\n",
      "1/1 - 0s - 98ms/step - accuracy: 0.1739 - loss: 2.8777\n",
      "Epoch 14/100\n",
      "1/1 - 0s - 66ms/step - accuracy: 0.1739 - loss: 2.8656\n",
      "Epoch 15/100\n",
      "1/1 - 0s - 89ms/step - accuracy: 0.1739 - loss: 2.8548\n",
      "Epoch 16/100\n",
      "1/1 - 0s - 116ms/step - accuracy: 0.1739 - loss: 2.8509\n",
      "Epoch 17/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.1739 - loss: 2.8298\n",
      "Epoch 18/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.1739 - loss: 2.8248\n",
      "Epoch 19/100\n",
      "1/1 - 0s - 60ms/step - accuracy: 0.1739 - loss: 2.8005\n",
      "Epoch 20/100\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.1739 - loss: 2.7841\n",
      "Epoch 21/100\n",
      "1/1 - 0s - 96ms/step - accuracy: 0.1739 - loss: 2.7549\n",
      "Epoch 22/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.1739 - loss: 2.7351\n",
      "Epoch 23/100\n",
      "1/1 - 0s - 83ms/step - accuracy: 0.1739 - loss: 2.7154\n",
      "Epoch 24/100\n",
      "1/1 - 0s - 82ms/step - accuracy: 0.1739 - loss: 2.6990\n",
      "Epoch 25/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.1739 - loss: 2.6818\n",
      "Epoch 26/100\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.1739 - loss: 2.6765\n",
      "Epoch 27/100\n",
      "1/1 - 0s - 65ms/step - accuracy: 0.1739 - loss: 2.6485\n",
      "Epoch 28/100\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.1739 - loss: 2.6405\n",
      "Epoch 29/100\n",
      "1/1 - 0s - 90ms/step - accuracy: 0.1739 - loss: 2.6009\n",
      "Epoch 30/100\n",
      "1/1 - 0s - 97ms/step - accuracy: 0.1739 - loss: 2.5759\n",
      "Epoch 31/100\n",
      "1/1 - 0s - 131ms/step - accuracy: 0.1739 - loss: 2.5485\n",
      "Epoch 32/100\n",
      "1/1 - 0s - 60ms/step - accuracy: 0.1739 - loss: 2.5784\n",
      "Epoch 33/100\n",
      "1/1 - 0s - 89ms/step - accuracy: 0.2174 - loss: 2.4951\n",
      "Epoch 34/100\n",
      "1/1 - 0s - 72ms/step - accuracy: 0.2174 - loss: 2.4847\n",
      "Epoch 35/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.2609 - loss: 2.4670\n",
      "Epoch 36/100\n",
      "1/1 - 0s - 71ms/step - accuracy: 0.2174 - loss: 2.4213\n",
      "Epoch 37/100\n",
      "1/1 - 0s - 88ms/step - accuracy: 0.1739 - loss: 2.3830\n",
      "Epoch 38/100\n",
      "1/1 - 0s - 82ms/step - accuracy: 0.1739 - loss: 2.3471\n",
      "Epoch 39/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.2174 - loss: 2.3078\n",
      "Epoch 40/100\n",
      "1/1 - 0s - 67ms/step - accuracy: 0.1739 - loss: 2.2634\n",
      "Epoch 41/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.1739 - loss: 2.2661\n",
      "Epoch 42/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.3043 - loss: 2.1654\n",
      "Epoch 43/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.2174 - loss: 2.1884\n",
      "Epoch 44/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.2174 - loss: 2.1510\n",
      "Epoch 45/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 0.2609 - loss: 2.1089\n",
      "Epoch 46/100\n",
      "1/1 - 0s - 92ms/step - accuracy: 0.2609 - loss: 2.0304\n",
      "Epoch 47/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.3478 - loss: 2.0489\n",
      "Epoch 48/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.2174 - loss: 2.0949\n",
      "Epoch 49/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.3043 - loss: 1.9440\n",
      "Epoch 50/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 0.2174 - loss: 2.0066\n",
      "Epoch 51/100\n",
      "1/1 - 0s - 67ms/step - accuracy: 0.3043 - loss: 1.8716\n",
      "Epoch 52/100\n",
      "1/1 - 0s - 72ms/step - accuracy: 0.3913 - loss: 1.8682\n",
      "Epoch 53/100\n",
      "1/1 - 0s - 83ms/step - accuracy: 0.4348 - loss: 1.7913\n",
      "Epoch 54/100\n",
      "1/1 - 0s - 92ms/step - accuracy: 0.4348 - loss: 1.7742\n",
      "Epoch 55/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.3043 - loss: 1.7863\n",
      "Epoch 56/100\n",
      "1/1 - 0s - 94ms/step - accuracy: 0.5217 - loss: 1.6937\n",
      "Epoch 57/100\n",
      "1/1 - 0s - 84ms/step - accuracy: 0.3478 - loss: 1.8073\n",
      "Epoch 58/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.3478 - loss: 1.7050\n",
      "Epoch 59/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 0.3913 - loss: 1.6984\n",
      "Epoch 60/100\n",
      "1/1 - 0s - 72ms/step - accuracy: 0.5217 - loss: 1.6241\n",
      "Epoch 61/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.4348 - loss: 1.6499\n",
      "Epoch 62/100\n",
      "1/1 - 0s - 87ms/step - accuracy: 0.4348 - loss: 1.5679\n",
      "Epoch 63/100\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.3478 - loss: 1.6491\n",
      "Epoch 64/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.4783 - loss: 1.5100\n",
      "Epoch 65/100\n",
      "1/1 - 0s - 91ms/step - accuracy: 0.5217 - loss: 1.5592\n",
      "Epoch 66/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.3913 - loss: 1.5178\n",
      "Epoch 67/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.5217 - loss: 1.4346\n",
      "Epoch 68/100\n",
      "1/1 - 0s - 68ms/step - accuracy: 0.5217 - loss: 1.4781\n",
      "Epoch 69/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.5652 - loss: 1.4973\n",
      "Epoch 70/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.4348 - loss: 1.3995\n",
      "Epoch 71/100\n",
      "1/1 - 0s - 62ms/step - accuracy: 0.6087 - loss: 1.3391\n",
      "Epoch 72/100\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4783 - loss: 1.4397\n",
      "Epoch 73/100\n",
      "1/1 - 0s - 96ms/step - accuracy: 0.5652 - loss: 1.2421\n",
      "Epoch 74/100\n",
      "1/1 - 0s - 92ms/step - accuracy: 0.6957 - loss: 1.2192\n",
      "Epoch 75/100\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.6957 - loss: 1.2835\n",
      "Epoch 76/100\n",
      "1/1 - 0s - 94ms/step - accuracy: 0.5652 - loss: 1.2315\n",
      "Epoch 77/100\n",
      "1/1 - 0s - 82ms/step - accuracy: 0.7391 - loss: 1.1528\n",
      "Epoch 78/100\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.5652 - loss: 1.1001\n",
      "Epoch 79/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.5652 - loss: 1.2147\n",
      "Epoch 80/100\n",
      "1/1 - 0s - 65ms/step - accuracy: 0.6087 - loss: 1.2262\n",
      "Epoch 81/100\n",
      "1/1 - 0s - 90ms/step - accuracy: 0.6957 - loss: 1.0919\n",
      "Epoch 82/100\n",
      "1/1 - 0s - 84ms/step - accuracy: 0.6522 - loss: 1.1096\n",
      "Epoch 83/100\n",
      "1/1 - 0s - 71ms/step - accuracy: 0.6522 - loss: 1.1143\n",
      "Epoch 84/100\n",
      "1/1 - 0s - 94ms/step - accuracy: 0.6522 - loss: 1.0975\n",
      "Epoch 85/100\n",
      "1/1 - 0s - 172ms/step - accuracy: 0.5652 - loss: 1.1512\n",
      "Epoch 86/100\n",
      "1/1 - 0s - 83ms/step - accuracy: 0.6522 - loss: 1.0566\n",
      "Epoch 87/100\n",
      "1/1 - 0s - 84ms/step - accuracy: 0.7391 - loss: 1.0523\n",
      "Epoch 88/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.7826 - loss: 1.0027\n",
      "Epoch 89/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.6957 - loss: 0.9061\n",
      "Epoch 90/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.6087 - loss: 1.0606\n",
      "Epoch 91/100\n",
      "1/1 - 0s - 100ms/step - accuracy: 0.5652 - loss: 1.0690\n",
      "Epoch 92/100\n",
      "1/1 - 0s - 124ms/step - accuracy: 0.7391 - loss: 0.9104\n",
      "Epoch 93/100\n",
      "1/1 - 0s - 82ms/step - accuracy: 0.7391 - loss: 0.9388\n",
      "Epoch 94/100\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.6957 - loss: 0.9896\n",
      "Epoch 95/100\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.6957 - loss: 0.9326\n",
      "Epoch 96/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.7391 - loss: 0.9083\n",
      "Epoch 97/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.7391 - loss: 0.9131\n",
      "Epoch 98/100\n",
      "1/1 - 0s - 97ms/step - accuracy: 0.6957 - loss: 0.8835\n",
      "Epoch 99/100\n",
      "1/1 - 0s - 90ms/step - accuracy: 0.7826 - loss: 0.9236\n",
      "Epoch 100/100\n",
      "1/1 - 0s - 90ms/step - accuracy: 0.7391 - loss: 0.8247\n",
      "Generated text: The sun rises in the east sky mat mat mat mat mat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "\n",
    "\n",
    "# Sample custom dataset (replace with your own data)\n",
    "data = [\n",
    "    \"The sun rises in the east\",\n",
    "    \"The moon shines at night\",\n",
    "    \"The stars twinkle in the sky\",\n",
    "    \"The earth orbits the sun\",\n",
    "    \"The cat sat on the mat\"\n",
    "]\n",
    "\n",
    "# Step 1: Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "total_words = len(tokenizer.word_index) + 1  # Include padding token\n",
    "\n",
    "# Generate input sequences\n",
    "input_sequences = []\n",
    "for line in data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences and create features/labels\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "X = input_sequences[:, :-1]\n",
    "y = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=total_words)\n",
    "\n",
    "# Step 2: Build the LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(total_words, 50, input_length=max_sequence_len - 1),\n",
    "    LSTM(150),\n",
    "    Dropout(0.2),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Step 3: Train the model\n",
    "history = model.fit(X, y, epochs=100, verbose=2)\n",
    "\n",
    "# Step 4: Generate text\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        next_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "        seed_text += \" \" + next_word\n",
    "    return seed_text\n",
    "\n",
    "# Example usage\n",
    "seed_text = \"The sun\"\n",
    "generated_text = generate_text(seed_text, next_words=10, model=model, max_sequence_len=max_sequence_len)\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ca2f87f-7212-44cb-bc98-098deb3aae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.Build a program that uses GPT-2 from Hugging Face to generate a story\n",
    "#based on a custom promptI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59cd86b5-4f3a-48dd-8c8e-9465b07ecf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (4.47.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages\\fault_detection-0.0.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohd yusuf haider\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c679b018-9a73-4121-9031-7d81ce8b6c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2400ba266441389433b84ee8788bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHD YUSUF HAIDER\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MOHD YUSUF HAIDER\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfcec102f9d7494cb72d7512085fcfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e591b8a7ffe4b5292290b77a5ddc1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3156274f83b348b39570b18ce7006357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f541b304d04870b1cd357a0a14e25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a64d59ce074ecdabd1da2e7908b6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd6651d681046dfbccb7e6daa2b6bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHD YUSUF HAIDER\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\MOHD YUSUF HAIDER\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "\n",
      "Once upon a time in a distant kingdom, there was a young prince who was the son of a nobleman. He was called the Prince of the House of King's Landing.\n",
      "\n",
      "The Prince was born in the year of Aes Sedai, and was raised in his father's house. His father was an old man, but he was very good at his craft. The Prince had a great deal of knowledge of magic, as well as of history. When he had been a child, he learned to read and write, to write and speak, so that he could write. But he did not know how to speak. So he began to learn to use magic. And when he came to the kingdom of Westeros, his mother was there, with her husband, the King of Winterfell. She was not a very well-educated woman, for she was only a little girl. Her father had died when she had grown up, when her father died, she grew up in an\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer from Hugging Face\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a story based on the custom prompt\n",
    "def generate_story(prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
    "    # Encode the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate the output sequence\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    story = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return story\n",
    "\n",
    "# Example custom prompt\n",
    "prompt = \"Once upon a time in a distant kingdom, there was a young prince who\"\n",
    "\n",
    "# Generate the story\n",
    "story = generate_story(prompt, max_length=200, temperature=0.7, top_p=0.9)\n",
    "\n",
    "# Print the generated story\n",
    "print(\"Generated Story:\\n\")\n",
    "print(story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ea3cea7-0889-4473-998b-895ce9d0a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.Write a code to implement a simple text generation model using a GRUbased architecture in Keras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "425119ee-1abe-49a1-a197-58b7dce7d06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 4s - 4s/step - accuracy: 0.0435 - loss: 2.9437\n",
      "Epoch 2/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.1739 - loss: 2.9369\n",
      "Epoch 3/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.3043 - loss: 2.9281\n",
      "Epoch 4/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.2609 - loss: 2.9177\n",
      "Epoch 5/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.2174 - loss: 2.9118\n",
      "Epoch 6/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.1739 - loss: 2.9066\n",
      "Epoch 7/100\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.2174 - loss: 2.8971\n",
      "Epoch 8/100\n",
      "1/1 - 0s - 85ms/step - accuracy: 0.2174 - loss: 2.8835\n",
      "Epoch 9/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 0.2174 - loss: 2.8760\n",
      "Epoch 10/100\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.1739 - loss: 2.8652\n",
      "Epoch 11/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.1739 - loss: 2.8604\n",
      "Epoch 12/100\n",
      "1/1 - 0s - 83ms/step - accuracy: 0.1739 - loss: 2.8479\n",
      "Epoch 13/100\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.1739 - loss: 2.8391\n",
      "Epoch 14/100\n",
      "1/1 - 0s - 66ms/step - accuracy: 0.1739 - loss: 2.8243\n",
      "Epoch 15/100\n",
      "1/1 - 0s - 71ms/step - accuracy: 0.1739 - loss: 2.8094\n",
      "Epoch 16/100\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.2174 - loss: 2.7996\n",
      "Epoch 17/100\n",
      "1/1 - 0s - 101ms/step - accuracy: 0.1739 - loss: 2.7746\n",
      "Epoch 18/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.1739 - loss: 2.7736\n",
      "Epoch 19/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.1739 - loss: 2.7471\n",
      "Epoch 20/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.2174 - loss: 2.7259\n",
      "Epoch 21/100\n",
      "1/1 - 0s - 83ms/step - accuracy: 0.1739 - loss: 2.7086\n",
      "Epoch 22/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.1739 - loss: 2.6891\n",
      "Epoch 23/100\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.1739 - loss: 2.6557\n",
      "Epoch 24/100\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.1739 - loss: 2.6209\n",
      "Epoch 25/100\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.1739 - loss: 2.6242\n",
      "Epoch 26/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.1739 - loss: 2.6178\n",
      "Epoch 27/100\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.1739 - loss: 2.6098\n",
      "Epoch 28/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.2174 - loss: 2.5403\n",
      "Epoch 29/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.1739 - loss: 2.5347\n",
      "Epoch 30/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.2174 - loss: 2.4760\n",
      "Epoch 31/100\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.2174 - loss: 2.4641\n",
      "Epoch 32/100\n",
      "1/1 - 0s - 66ms/step - accuracy: 0.2174 - loss: 2.4211\n",
      "Epoch 33/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.2174 - loss: 2.3848\n",
      "Epoch 34/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.1739 - loss: 2.3354\n",
      "Epoch 35/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.3043 - loss: 2.3333\n",
      "Epoch 36/100\n",
      "1/1 - 0s - 66ms/step - accuracy: 0.1739 - loss: 2.2812\n",
      "Epoch 37/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.2174 - loss: 2.2357\n",
      "Epoch 38/100\n",
      "1/1 - 0s - 66ms/step - accuracy: 0.1739 - loss: 2.1919\n",
      "Epoch 39/100\n",
      "1/1 - 0s - 68ms/step - accuracy: 0.2174 - loss: 2.1522\n",
      "Epoch 40/100\n",
      "1/1 - 0s - 67ms/step - accuracy: 0.2174 - loss: 2.0669\n",
      "Epoch 41/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.2609 - loss: 2.0215\n",
      "Epoch 42/100\n",
      "1/1 - 0s - 68ms/step - accuracy: 0.2174 - loss: 2.0345\n",
      "Epoch 43/100\n",
      "1/1 - 0s - 71ms/step - accuracy: 0.2174 - loss: 1.9556\n",
      "Epoch 44/100\n",
      "1/1 - 0s - 68ms/step - accuracy: 0.1739 - loss: 1.9284\n",
      "Epoch 45/100\n",
      "1/1 - 0s - 97ms/step - accuracy: 0.3043 - loss: 1.8261\n",
      "Epoch 46/100\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.2174 - loss: 1.8042\n",
      "Epoch 47/100\n",
      "1/1 - 0s - 63ms/step - accuracy: 0.3478 - loss: 1.8013\n",
      "Epoch 48/100\n",
      "1/1 - 0s - 65ms/step - accuracy: 0.3913 - loss: 1.7379\n",
      "Epoch 49/100\n",
      "1/1 - 0s - 64ms/step - accuracy: 0.2609 - loss: 1.7550\n",
      "Epoch 50/100\n",
      "1/1 - 0s - 66ms/step - accuracy: 0.4783 - loss: 1.6828\n",
      "Epoch 51/100\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.5652 - loss: 1.5800\n",
      "Epoch 52/100\n",
      "1/1 - 0s - 68ms/step - accuracy: 0.5652 - loss: 1.5869\n",
      "Epoch 53/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.6522 - loss: 1.5175\n",
      "Epoch 54/100\n",
      "1/1 - 0s - 67ms/step - accuracy: 0.6087 - loss: 1.4823\n",
      "Epoch 55/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.6522 - loss: 1.4481\n",
      "Epoch 56/100\n",
      "1/1 - 0s - 68ms/step - accuracy: 0.6522 - loss: 1.3602\n",
      "Epoch 57/100\n",
      "1/1 - 0s - 86ms/step - accuracy: 0.5652 - loss: 1.4687\n",
      "Epoch 58/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.6522 - loss: 1.3334\n",
      "Epoch 59/100\n",
      "1/1 - 0s - 68ms/step - accuracy: 0.6087 - loss: 1.3378\n",
      "Epoch 60/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.5652 - loss: 1.2575\n",
      "Epoch 61/100\n",
      "1/1 - 0s - 67ms/step - accuracy: 0.5217 - loss: 1.2630\n",
      "Epoch 62/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.5217 - loss: 1.3354\n",
      "Epoch 63/100\n",
      "1/1 - 0s - 63ms/step - accuracy: 0.6522 - loss: 1.2474\n",
      "Epoch 64/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.8261 - loss: 1.1036\n",
      "Epoch 65/100\n",
      "1/1 - 0s - 68ms/step - accuracy: 0.7391 - loss: 1.1294\n",
      "Epoch 66/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.6957 - loss: 1.1469\n",
      "Epoch 67/100\n",
      "1/1 - 0s - 72ms/step - accuracy: 0.7391 - loss: 0.9488\n",
      "Epoch 68/100\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.7826 - loss: 0.8927\n",
      "Epoch 69/100\n",
      "1/1 - 0s - 102ms/step - accuracy: 0.7826 - loss: 0.9815\n",
      "Epoch 70/100\n",
      "1/1 - 0s - 66ms/step - accuracy: 0.8261 - loss: 0.9092\n",
      "Epoch 71/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.6957 - loss: 1.0039\n",
      "Epoch 72/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.7826 - loss: 0.8919\n",
      "Epoch 73/100\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.7826 - loss: 0.8829\n",
      "Epoch 74/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.7391 - loss: 0.9002\n",
      "Epoch 75/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.7826 - loss: 0.9411\n",
      "Epoch 76/100\n",
      "1/1 - 0s - 68ms/step - accuracy: 0.7391 - loss: 0.8960\n",
      "Epoch 77/100\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.8696 - loss: 0.8094\n",
      "Epoch 78/100\n",
      "1/1 - 0s - 61ms/step - accuracy: 0.7826 - loss: 0.7658\n",
      "Epoch 79/100\n",
      "1/1 - 0s - 60ms/step - accuracy: 0.6957 - loss: 0.8024\n",
      "Epoch 80/100\n",
      "1/1 - 0s - 66ms/step - accuracy: 0.7826 - loss: 0.8056\n",
      "Epoch 81/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.8261 - loss: 0.7066\n",
      "Epoch 82/100\n",
      "1/1 - 0s - 58ms/step - accuracy: 0.8696 - loss: 0.7209\n",
      "Epoch 83/100\n",
      "1/1 - 0s - 61ms/step - accuracy: 0.7826 - loss: 0.7096\n",
      "Epoch 84/100\n",
      "1/1 - 0s - 61ms/step - accuracy: 0.7391 - loss: 0.6891\n",
      "Epoch 85/100\n",
      "1/1 - 0s - 63ms/step - accuracy: 0.7826 - loss: 0.7667\n",
      "Epoch 86/100\n",
      "1/1 - 0s - 65ms/step - accuracy: 0.7826 - loss: 0.6957\n",
      "Epoch 87/100\n",
      "1/1 - 0s - 72ms/step - accuracy: 0.7826 - loss: 0.7275\n",
      "Epoch 88/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.7826 - loss: 0.6254\n",
      "Epoch 89/100\n",
      "1/1 - 0s - 70ms/step - accuracy: 0.8696 - loss: 0.6514\n",
      "Epoch 90/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.8261 - loss: 0.5645\n",
      "Epoch 91/100\n",
      "1/1 - 0s - 71ms/step - accuracy: 0.7826 - loss: 0.5995\n",
      "Epoch 92/100\n",
      "1/1 - 0s - 69ms/step - accuracy: 0.8261 - loss: 0.5832\n",
      "Epoch 93/100\n",
      "1/1 - 0s - 71ms/step - accuracy: 0.7391 - loss: 0.5640\n",
      "Epoch 94/100\n",
      "1/1 - 0s - 64ms/step - accuracy: 0.7826 - loss: 0.5442\n",
      "Epoch 95/100\n",
      "1/1 - 0s - 65ms/step - accuracy: 0.7826 - loss: 0.5602\n",
      "Epoch 96/100\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.7391 - loss: 0.6060\n",
      "Epoch 97/100\n",
      "1/1 - 0s - 68ms/step - accuracy: 0.6957 - loss: 0.6056\n",
      "Epoch 98/100\n",
      "1/1 - 0s - 67ms/step - accuracy: 0.8261 - loss: 0.6134\n",
      "Epoch 99/100\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.7826 - loss: 0.5760\n",
      "Epoch 100/100\n",
      "1/1 - 0s - 67ms/step - accuracy: 0.8696 - loss: 0.5086\n",
      "Generated text:\n",
      "\n",
      "The sun rises in the east east east sky east sky sky\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.layers import GRU, Embedding, Dense, Dropout\n",
    "\n",
    "\n",
    "\n",
    "data = [\n",
    "    \"The sun rises in the east\",\n",
    "    \"The moon shines at night\",\n",
    "    \"The stars twinkle in the sky\",\n",
    "    \"The earth orbits the sun\",\n",
    "    \"The cat sat on the mat\"\n",
    "]\n",
    "\n",
    "# Step 1: Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "total_words = len(tokenizer.word_index) + 1  \n",
    "\n",
    "# Generate input sequences\n",
    "input_sequences = []\n",
    "for line in data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences and create features/labels\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "X = input_sequences[:, :-1]\n",
    "y = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=total_words)\n",
    "\n",
    "# Step 2: Build the GRU model\n",
    "# Step 2: Build the GRU model\n",
    "model = Sequential([\n",
    "    Embedding(total_words, 50, input_length=max_sequence_len - 1),  # Remove input_length argument\n",
    "    GRU(150),\n",
    "    Dropout(0.2),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Step 3: Train the model\n",
    "history = model.fit(X, y, epochs=100, verbose=2)\n",
    "\n",
    "# Step 4: Generate text\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        next_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "        seed_text += \" \" + next_word\n",
    "    return seed_text\n",
    "\n",
    "# Example usage\n",
    "seed_text = \"The sun\"\n",
    "generated_text = generate_text(seed_text, next_words=10, model=model, max_sequence_len=max_sequence_len)\n",
    "print(\"Generated text:\\n\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc1d8053-e3c8-4173-8310-8f4466b6bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.I Create a script to implement GPT-2-based text generation with beam\n",
    "#search decoding to generate text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7eecfe37-cd69-4481-8d16-f46322c31fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story with Beam Search:\n",
      "\n",
      "Once upon a time in a distant kingdom, there was a brave knight who fought valiantly against the enemy.\n",
      "\n",
      "He was the son of a noble nobleman, and he had been knighted by his father. He had fought in the Battle of the Bastille, in which he lost his life. His father had said to him, \"If you wish to become a knight, you must be able to do so. If you do not, then you will not become one. You will\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer from Hugging Face\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate text with beam search decoding\n",
    "def generate_text_with_beam_search(prompt, num_beams=5, max_length=100, temperature=1.0, no_repeat_ngram_size=2):\n",
    "    # Encode the input prompt to tokens\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate text using beam search\n",
    "    beam_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the output sequence to text\n",
    "    generated_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Once upon a time in a distant kingdom, there was a brave knight who\"\n",
    "generated_text = generate_text_with_beam_search(prompt, num_beams=5, max_length=100)\n",
    "\n",
    "# Print the generated story\n",
    "print(\"Generated Story with Beam Search:\\n\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "405003ee-3ee3-42d7-8539-9658f4af6a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.Implement a text generation script using GPT-2 with a custom temperature\n",
    "#setting for diversity in output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2b4220b-32db-498c-bb34-60c645f3379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " In the future, AI will also learn how to work with computers to find patterns in objects. That will help us in developing a better understanding of why human beings behave differently and why we have different emotions.\n",
      "\n",
      "We will then start to learn more about the behavior of the human brain in everyday life. This will enable us to better understand how we are different and what we need to do to achieve what humans need. The future of human behavior will involve better tools and tools for developing better ways of\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def generate_text_with_temperature(prompt, temperature=1.0, max_length=100):\n",
    "  \n",
    "    # Load pre-trained GPT-2 model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Encode the prompt text to get token IDs\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate text with the specified temperature\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2, \n",
    "        top_p=0.92,  \n",
    "        top_k=50, \n",
    "        do_sample=True,  \n",
    "        pad_token_id=tokenizer.eos_token_id  \n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens to human-readable text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"In the future, AI will\"\n",
    "temperature = 0.8  \n",
    "generated_text = generate_text_with_temperature(prompt, temperature, max_length=100)\n",
    "\n",
    "print(\"Generated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5404b9f-e585-4a07-b590-25da599d9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.Create a script to implement temperature sampling with GPT-2,\n",
    "#experimenting with different values to generate creative text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "261b8bed-51c8-48aa-9990-ee9854fc8894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The future of artificial intelligence\n",
      "--------------------------------------------------\n",
      "Temperature: 0.5\n",
      "Generated Text:\n",
      "The future of artificial intelligence is uncertain. The future is still a mystery.\n",
      "\n",
      "The Future of Artificial Intelligence\n",
      ". . .\n",
      ", A. A., A, L., & G. Gómez-Bruno. (2015). The Future Of Artificial intelligence: A Critical Review. Retrieved from http://www.sciencedirect.com/science/article/pii/S00251439005908/full\n",
      " and http/pdf/Abstract/A_\n",
      "--------------------------------------------------\n",
      "Temperature: 1.0\n",
      "Generated Text:\n",
      "The future of artificial intelligence? Will it really make humanity as better as possible, and will it eventually win?\n",
      "\n",
      "This answer could have a huge impact on how we think about life. But the best answers don't always lie within our immediate future, which is why we should have this conversation before we do our business.\n",
      "/p/k/\n",
      "- - -\n",
      "[Note: The author's first name is Patrick.]\n",
      "I know that this post will sound too complicated, but please\n",
      "--------------------------------------------------\n",
      "Temperature: 1.5\n",
      "Generated Text:\n",
      "The future of artificial intelligence\n",
      "\n",
      "By Bruce Schneier May 11\n",
      "... the U.K.'s chief academic officer has stated in an online message that we would become irrelevant as technology increases as we \"get closer... we are also going through the period of fundamental automation.\" That may happen within a couple of years depending on the amount of effort and information we gather, according to David Greenhalgh, the UK's director of scientific data management for Digital L.C. There is \"an emerging\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "def generate_text_with_temperature(prompt, temperature=1.0, max_length=100):\n",
    " \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Encode the input prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate text using the specified temperature\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=1,\n",
    "        top_p=0.92,  \n",
    "        top_k=50,   \n",
    "        do_sample=True, \n",
    "        no_repeat_ngram_size=2,  \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Experiment with different temperatures\n",
    "def experiment_with_temperatures(prompt, temperatures, max_length=100):\n",
    "\n",
    "    print(f\"Prompt: {prompt}\\n{'-'*50}\")\n",
    "    for temp in temperatures:\n",
    "        print(f\"Temperature: {temp}\")\n",
    "        generated_text = generate_text_with_temperature(prompt, temperature=temp, max_length=max_length)\n",
    "        print(f\"Generated Text:\\n{generated_text}\\n{'-'*50}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"The future of artificial intelligence\"\n",
    "    temperatures = [0.5, 1.0, 1.5]  \n",
    "    max_length = 100\n",
    "\n",
    "    experiment_with_temperatures(prompt, temperatures, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d55444b-721e-4055-9d75-e246ba069d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.I How can you implement text generation using it in a simple custom\n",
    "#attention-based architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1722b6f9-29e6-4cff-bdc9-f2373c26af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention\n",
    "\n",
    "# Sample data: input-output sentence pairs (toy dataset)\n",
    "data = [\n",
    "    (\"the sky is blue\", \"el cielo es azul\"),\n",
    "    (\"the sun is bright\", \"el sol es brillante\"),\n",
    "    (\"the grass is green\", \"la hierba es verde\"),\n",
    "]\n",
    "\n",
    "# Preprocess the data\n",
    "input_texts = [pair[0] for pair in data]\n",
    "output_texts = [\"<start> \" + pair[1] + \" <end>\" for pair in data]\n",
    "\n",
    "# Tokenize the input and output texts\n",
    "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "\n",
    "output_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "output_tokenizer.fit_on_texts(output_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(output_texts)\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_input_len = max(len(seq) for seq in input_sequences)\n",
    "max_output_len = max(len(seq) for seq in output_sequences)\n",
    "\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
    "output_sequences = tf.keras.preprocessing.sequence.pad_sequences(output_sequences, maxlen=max_output_len, padding='post')\n",
    "\n",
    "# Prepare features and labels for training\n",
    "encoder_input_data = input_sequences\n",
    "decoder_input_data = output_sequences[:, :-1]\n",
    "decoder_target_data = output_sequences[:, 1:]\n",
    "\n",
    "# Build the Encoder-Decoder Model with Attention\n",
    "latent_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_len,))\n",
    "encoder_embedding = Embedding(input_vocab_size, latent_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_output_len - 1,))\n",
    "decoder_embedding = Embedding(output_vocab_size, latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention mechanism\n",
    "attention = Attention()\n",
    "context_vector = attention([decoder_outputs, encoder_outputs])\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "decoder_combined_context = Concatenate(axis=-1)([decoder_outputs, context_vector])\n",
    "\n",
    "\n",
    "# Dense output layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e7c10-119e-437a-95ea-13a9cb06f771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
